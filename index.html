<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Rick O. Gilmore" />
  <title>The Video DatAbservatory: A platform for behavioral discovery</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="libs/reveal.js-3.3.0.1/css/reveal.css"/>



<link rel="stylesheet" href="libs/reveal.js-3.3.0.1/css/theme/simple.css" id="theme">


  <!-- some tweaks to reveal css -->
  <style type="text/css">
    .reveal h1 { font-size: 2.0em; }
    .reveal h2 { font-size: 1.5em;  }
    .reveal h3 { font-size: 1.25em;	}
    .reveal h4 { font-size: 1em;	}

    .reveal .slides>section,
    .reveal .slides>section>section {
      padding: 0px 0px;
    }



    .reveal table {
      border-width: 1px;
      border-spacing: 2px;
      border-style: dotted;
      border-color: gray;
      border-collapse: collapse;
      font-size: 0.7em;
    }

    .reveal table th {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      font-weight: bold;
      border-style: dotted;
      border-color: gray;
    }

    .reveal table td {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      border-style: dotted;
      border-color: gray;
    }


  </style>

    <style type="text/css">code{white-space: pre;}</style>


<!-- Printing and PDF exports -->
<script id="paper-css" type="application/dynamic-css">

/* Default Print Stylesheet Template
   by Rob Glazebrook of CSSnewbie.com
   Last Updated: June 4, 2008

   Feel free (nay, compelled) to edit, append, and
   manipulate this file as you see fit. */


@media print {

	/* SECTION 1: Set default width, margin, float, and
	   background. This prevents elements from extending
	   beyond the edge of the printed page, and prevents
	   unnecessary background images from printing */
	html {
		background: #fff;
		width: auto;
		height: auto;
		overflow: visible;
	}
	body {
		background: #fff;
		font-size: 20pt;
		width: auto;
		height: auto;
		border: 0;
		margin: 0 5%;
		padding: 0;
		overflow: visible;
		float: none !important;
	}

	/* SECTION 2: Remove any elements not needed in print.
	   This would include navigation, ads, sidebars, etc. */
	.nestedarrow,
	.controls,
	.fork-reveal,
	.share-reveal,
	.state-background,
	.reveal .progress,
	.reveal .backgrounds {
		display: none !important;
	}

	/* SECTION 3: Set body font face, size, and color.
	   Consider using a serif font for readability. */
	body, p, td, li, div {
		font-size: 20pt!important;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		color: #000;
	}

	/* SECTION 4: Set heading font face, sizes, and color.
	   Differentiate your headings from your body text.
	   Perhaps use a large sans-serif for distinction. */
	h1,h2,h3,h4,h5,h6 {
		color: #000!important;
		height: auto;
		line-height: normal;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		text-shadow: 0 0 0 #000 !important;
		text-align: left;
		letter-spacing: normal;
	}
	/* Need to reduce the size of the fonts for printing */
	h1 { font-size: 28pt !important;  }
	h2 { font-size: 24pt !important; }
	h3 { font-size: 22pt !important; }
	h4 { font-size: 22pt !important; font-variant: small-caps; }
	h5 { font-size: 21pt !important; }
	h6 { font-size: 20pt !important; font-style: italic; }

	/* SECTION 5: Make hyperlinks more usable.
	   Ensure links are underlined, and consider appending
	   the URL to the end of the link for usability. */
	a:link,
	a:visited {
		color: #000 !important;
		font-weight: bold;
		text-decoration: underline;
	}
	/*
	.reveal a:link:after,
	.reveal a:visited:after {
		content: " (" attr(href) ") ";
		color: #222 !important;
		font-size: 90%;
	}
	*/


	/* SECTION 6: more reveal.js specific additions by @skypanther */
	ul, ol, div, p {
		visibility: visible;
		position: static;
		width: auto;
		height: auto;
		display: block;
		overflow: visible;
		margin: 0;
		text-align: left !important;
	}
	.reveal pre,
	.reveal table {
		margin-left: 0;
		margin-right: 0;
	}
	.reveal pre code {
		padding: 20px;
		border: 1px solid #ddd;
	}
	.reveal blockquote {
		margin: 20px 0;
	}
	.reveal .slides {
		position: static !important;
		width: auto !important;
		height: auto !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 0 !important;
		zoom: 1 !important;

		overflow: visible !important;
		display: block !important;

		text-align: left !important;
		-webkit-perspective: none;
		   -moz-perspective: none;
		    -ms-perspective: none;
		        perspective: none;

		-webkit-perspective-origin: 50% 50%;
		   -moz-perspective-origin: 50% 50%;
		    -ms-perspective-origin: 50% 50%;
		        perspective-origin: 50% 50%;
	}
	.reveal .slides section {
		visibility: visible !important;
		position: static !important;
		width: auto !important;
		height: auto !important;
		display: block !important;
		overflow: visible !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 60px 20px !important;
		z-index: auto !important;

		opacity: 1 !important;

		page-break-after: always !important;

		-webkit-transform-style: flat !important;
		   -moz-transform-style: flat !important;
		    -ms-transform-style: flat !important;
		        transform-style: flat !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;

		-webkit-transition: none !important;
		   -moz-transition: none !important;
		    -ms-transition: none !important;
		        transition: none !important;
	}
	.reveal .slides section.stack {
		padding: 0 !important;
	}
	.reveal section:last-of-type {
		page-break-after: avoid !important;
	}
	.reveal section .fragment {
		opacity: 1 !important;
		visibility: visible !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;
	}
	.reveal section img {
		display: block;
		margin: 15px 0px;
		background: rgba(255,255,255,1);
		border: 1px solid #666;
		box-shadow: none;
	}

	.reveal section small {
		font-size: 0.8em;
	}

}  
</script>


<script id="pdf-css" type="application/dynamic-css">
    
/**
 * This stylesheet is used to print reveal.js
 * presentations to PDF.
 *
 * https://github.com/hakimel/reveal.js#pdf-export
 */

* {
	-webkit-print-color-adjust: exact;
}

body {
	margin: 0 auto !important;
	border: 0;
	padding: 0;
	float: none !important;
	overflow: visible;
}

html {
	width: 100%;
	height: 100%;
	overflow: visible;
}

/* Remove any elements not needed in print. */
.nestedarrow,
.reveal .controls,
.reveal .progress,
.reveal .playback,
.reveal.overview,
.fork-reveal,
.share-reveal,
.state-background {
	display: none !important;
}

h1, h2, h3, h4, h5, h6 {
	text-shadow: 0 0 0 #000 !important;
}

.reveal pre code {
	overflow: hidden !important;
	font-family: Courier, 'Courier New', monospace !important;
}

ul, ol, div, p {
	visibility: visible;
	position: static;
	width: auto;
	height: auto;
	display: block;
	overflow: visible;
	margin: auto;
}
.reveal {
	width: auto !important;
	height: auto !important;
	overflow: hidden !important;
}
.reveal .slides {
	position: static;
	width: 100%;
	height: auto;

	left: auto;
	top: auto;
	margin: 0 !important;
	padding: 0 !important;

	overflow: visible;
	display: block;

	-webkit-perspective: none;
	   -moz-perspective: none;
	    -ms-perspective: none;
	        perspective: none;

	-webkit-perspective-origin: 50% 50%; /* there isn't a none/auto value but 50-50 is the default */
	   -moz-perspective-origin: 50% 50%;
	    -ms-perspective-origin: 50% 50%;
	        perspective-origin: 50% 50%;
}

.reveal .slides section {
	page-break-after: always !important;

	visibility: visible !important;
	position: relative !important;
	display: block !important;
	position: relative !important;

	margin: 0 !important;
	padding: 0 !important;
	box-sizing: border-box !important;
	min-height: 1px;

	opacity: 1 !important;

	-webkit-transform-style: flat !important;
	   -moz-transform-style: flat !important;
	    -ms-transform-style: flat !important;
	        transform-style: flat !important;

	-webkit-transform: none !important;
	   -moz-transform: none !important;
	    -ms-transform: none !important;
	        transform: none !important;
}

.reveal section.stack {
	margin: 0 !important;
	padding: 0 !important;
	page-break-after: avoid !important;
	height: auto !important;
	min-height: auto !important;
}

.reveal img {
	box-shadow: none;
}

.reveal .roll {
	overflow: visible;
	line-height: 1em;
}

/* Slide backgrounds are placed inside of their slide when exporting to PDF */
.reveal section .slide-background {
	display: block !important;
	position: absolute;
	top: 0;
	left: 0;
	width: 100%;
	z-index: -1;
}

/* All elements should be above the slide-background */
.reveal section>* {
	position: relative;
	z-index: 1;
}

/* Display slide speaker notes when 'showNotes' is enabled */
.reveal .speaker-notes-pdf {
	display: block;
	width: 100%;
	max-height: none;
	left: auto;
	top: auto;
	z-index: 100;
}

/* Display slide numbers when 'slideNumber' is enabled */
.reveal .slide-number-pdf {
	display: block;
	position: absolute;
	font-size: 14px;
}

</script>


<script>
var style = document.createElement( 'style' );
style.type = 'text/css';
var style_script_id = window.location.search.match( /print-pdf/gi ) ? 'pdf-css' : 'paper-css';
var style_script = document.getElementById(style_script_id).text;
style.innerHTML = style_script;
document.getElementsByTagName('head')[0].appendChild(style);
</script>

</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">The Video DatAbservatory: A platform for behavioral discovery</h1>
    <h2 class="author">Rick O. Gilmore</h2>
    <h3 class="date">2018-01-26 08:50:37</h3>
</section>

<section><section id="support" class="titleslide slide level1"><h1>Support</h1></section><section class="slide level2">

<div class="centered">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/8/87/NSF_Logo.PNG" height=250px> <img src="https://science.nichd.nih.gov/confluence/download/attachments/34472103/NICHD-vertical-2-color.png?version=1&modificationDate=1477410070000&api=v2" height=250px> </br> <img src="https://sloan.org/storage/app/media/Logos/Sloan-Logo-stacked-black-web.png" height-250px> <img src="http://sagebase.org/wp-content/uploads/2016/03/sponsor_NIHmentalhealth.jpg" height=250px></p>
</div>
</section></section>
<section><section id="agenda" class="titleslide slide level1"><h1>Agenda</h1></section><section class="slide level2">

<h3 id="surveydiscussion">Survey/discussion</h3>
<h3 id="status-report">Status report</h3>
<h3 id="new-initiatives">New initiative(s)</h3>
</section></section>
<section><section id="surveydiscussion-1" class="titleslide slide level1" data-background="#999999"><h1>Survey/discussion</h1></section><section id="developmental-science-could-be-more-open-transparent" class="slide level2">
<h2>Developmental science could be more open &amp; transparent</h2>
</section><section class="slide level2">

<h3 id="agree">Agree</h3>
<h3 id="disagree">Disagree</h3>
</section><section id="developmental-science-should-be-more-open-and-transparent" class="slide level2">
<h2>Developmental science <em>should</em> be more open and transparent</h2>
</section><section class="slide level2">

<h3 id="agree-1">Agree</h3>
<h3 id="disagree-1">Disagree</h3>
</section><section id="openness-and-transparency-are-related-to-research-robustness-e.g.-reproducibility-reliability-impact" class="slide level2">
<h2>Openness and transparency are related to research robustness (e.g., reproducibility, reliability, impact)</h2>
</section><section class="slide level2">

<h3 id="agree-2">Agree</h3>
<h3 id="disagree-2">Disagree</h3>
</section><section id="data-from-developmental-research-should-be-more-widely-and-readily-available" class="slide level2">
<h2>Data from developmental research should be more widely and readily available</h2>
</section><section class="slide level2">

<h3 id="agree-3">Agree</h3>
<h3 id="disagree-3">Disagree</h3>
</section><section id="methods-and-materials-used-in-developmental-research-should-be-more-widely-and-readily-available" class="slide level2">
<h2>Methods and materials used in developmental research should be more widely and readily available</h2>
</section><section class="slide level2">

<h3 id="agree-4">Agree</h3>
<h3 id="disagree-4">Disagree</h3>
</section><section id="i-have-used-data-shared-by-others" class="slide level2">
<h2>I have used data shared by others</h2>
</section><section class="slide level2">

<h3 id="agree-5">Agree</h3>
<h3 id="disagree-5">Disagree</h3>
</section><section id="if-data-from-publication-x-or-project-y-were-more-widely-and-readily-available-i-would-use-it" class="slide level2">
<h2>If data from publication X or project Y were more widely and readily available, I would use it</h2>
</section><section class="slide level2">

<h3 id="agree-6">Agree</h3>
<h3 id="disagree-6">Disagree</h3>
</section><section id="unless-there-are-privacy-or-contractual-limitations-data-files-described-in-published-papers-should-be-readily-available-in-forms-reusable-by-others" class="slide level2">
<h2>Unless there are privacy or contractual limitations, data files described in published papers should be readily available in forms reusable by others</h2>
</section><section class="slide level2">

<h3 id="agree-7">Agree</h3>
<h3 id="disagree-7">Disagree</h3>
</section><section id="i-use-video-or-audio-recordings-in-my-teaching" class="slide level2">
<h2>I use video or audio recordings in my teaching</h2>
</section><section class="slide level2">

<h3 id="agree-8">Agree</h3>
<h3 id="disagree-8">Disagree</h3>
</section><section id="i-use-video-or-audio-recordings-in-my-current-research" class="slide level2">
<h2>I use video or audio recordings in my current research</h2>
</section><section class="slide level2">

<h3 id="agree-9">Agree</h3>
<h3 id="disagree-9">Disagree</h3>
</section><section id="i-can-imagine-using-video-or-audio-recordings-in-my-research" class="slide level2">
<h2>I can imagine using video or audio recordings in my research</h2>
</section><section class="slide level2">

<h3 id="agree-10">Agree</h3>
<h3 id="disagree-10">Disagree</h3>
</section><section id="i-use-video-or-audio-recordings-to-document-my-research-procedures" class="slide level2">
<h2>I use video or audio recordings to document my research procedures</h2>
</section><section class="slide level2">

<h3 id="agree-11">Agree</h3>
<h3 id="disagree-11">Disagree</h3>
</section><section id="i-could-envision-using-video-or-audio-recordings-to-document-my-research-procedures" class="slide level2">
<h2>I could envision using video or audio recordings to document my research procedures</h2>
</section><section class="slide level2">

<h3 id="agree-12">Agree</h3>
<h3 id="disagree-12">Disagree</h3>
</section><section id="video-and-audio-recordings-require-extensive-and-expensive-post-processing-and-coding" class="slide level2">
<h2>Video and audio recordings require extensive and expensive post-processing and coding</h2>
</section><section class="slide level2">

<h3 id="agree-13">Agree</h3>
<h3 id="disagree-13">Disagree</h3>
</section><section id="its-hard-to-find-and-access-data-that-i-might-want-to-repurpose" class="slide level2">
<h2>It’s hard to find and access data that I might want to repurpose</h2>
</section><section class="slide level2">

<h3 id="agree-14">Agree</h3>
<h3 id="disagree-14">Disagree</h3>
</section><section id="once-found-and-accessed-there-can-be-a-huge-cost-in-harmonizing-data-from-different-sources" class="slide level2">
<h2>Once found and accessed, there can be a huge cost in “harmonizing” data from different sources</h2>
</section><section class="slide level2">

<h3 id="agree-15">Agree</h3>
<h3 id="disagree-15">Disagree</h3>
</section><section id="developments-in-machine-learning-computer-vision-and-related-fields-are-interesting-to-me" class="slide level2">
<h2>Developments in machine learning, computer vision and related fields are interesting to me</h2>
</section><section class="slide level2">

<h3 id="agree-16">Agree</h3>
<h3 id="disagree-16">Disagree</h3>
</section><section id="i-would-be-interested-in-using-machine-learning-computer-vision-or-related-tools-in-my-research-under-the-right-circumstances" class="slide level2">
<h2>I would be interested in using machine learning, computer vision, or related tools in my research under the right circumstances</h2>
</section><section class="slide level2">

<h3 id="agree-17">Agree</h3>
<h3 id="disagree-17">Disagree</h3>
<!-- ## I employ reproducible practices and tools (e.g., SPSS or SAS syntax, R code, Jupyter notebooks) in my research workflows -->
<!-- --- -->
<!-- ### Agree -->
<!-- ### Disagree -->
</section></section>
<section><section id="status-update" class="titleslide slide level1" data-background="#999999"><h1>Status update</h1></section><section class="slide level2">

<p><a href="http://databrary.org"> <img src="https://nyu.databrary.org/web/images/logo/databrary-nav.svg" height=200px/> </a></p>
<p>Funded NSF (2012-16), NICHD (2013-18), &amp; Sloan Fdn (2017-18)</p>
<p>Opened spring 2014</p>
<p>Approaching 1,000 researchers (~680 PIs + ~290 affiliates)</p>
<p>500+ data/stimulus sets (~20% shared), 13,700+ hours</p>
</section><section class="slide level2">

<p><img src="http://datavyu.org/theme/img/logo/datavyu.png" height=150px/></p>
<p>Free, open-source, multi-platform video/audio coding tool</p>
<p>Windows OS fix nearly complete</p>
<p>Updates for transcription</p>
</section><section id="play-learning-across-a-year-play-project" class="slide level2">
<h2>Play &amp; Learning Across a Year (PLAY) Project</h2>
</section><section class="slide level2">

<h3 id="play-is-the-central-context-and-activity-of-early-development">Play is the central context and activity of early development</h3>
</section><section class="slide level2">

<h3 id="what-do-parents-and-infants-actually-do-when-they-play">What do parents and infants actually <em>do</em> when they play?</h3>
</section><section class="slide level2">

<div class="centered">
<video width="750" height="500" controls>
<source src="https://nyu.databrary.org/slot/11652/307774,376273/asset/47075/download?inline=true" type="video/mp4"> Your browser does not support the video tag.
</video>
<p>Adolph, K., Tamis-LeMonda, C. &amp; Gilmore, R.O. (2016). PLAY Project: Webinar discussions on protocol and coding. Databrary. Retrieved January 24, 2018 from <a href="https://nyu.databrary.org/volume/232" class="uri">https://nyu.databrary.org/volume/232</a></p>
</div>
</section><section class="slide level2">

<div class="centered">
<video width="750" height="500" controls>
<source src="https://nyu.databrary.org/slot/14167/381504,4603862/asset/59930/download?inline=true" type="video/mp4"> Your browser does not support the video tag.
</video>
<p>Adolph, K., Tamis-LeMonda, C. &amp; Gilmore, R.O. (2016). PLAY Project: Materials. Databrary. Retrieved January 24, 2018 from <a href="https://nyu.databrary.org/volume/254" class="uri">https://nyu.databrary.org/volume/254</a>.</p>
</div>
</section><section class="slide level2">

<ul>
<li><span class="math inline">\(n=900\)</span> infant/mother dyads; 300 @ 12-, 18-, 24-months</li>
<li>30 dyads from 30 sites across the US</li>
<li>1 hr natural activity
<ul>
<li>3 min solitary toy play</li>
<li>2 min dyadic toy play</li>
<li>video tour of home</li>
</ul></li>
</ul>
</section><section class="slide level2">

<ul>
<li>Videos coded for
<ul>
<li><a href="https://dev1.ed-projects.nyu.edu/wikis/docuwiki/doku.php/emotion">Emotional expression</a></li>
<li><a href="https://dev1.ed-projects.nyu.edu/wikis/docuwiki/doku.php/manual3">Object interaction</a></li>
<li>Physical activity &amp; <a href="https://dev1.ed-projects.nyu.edu/wikis/docuwiki/doku.php/manual4">locomotion</a></li>
<li><a href="https://dev1.ed-projects.nyu.edu/wikis/docuwiki/doku.php/transcription">Full transcript</a>, <a href="https://dev1.ed-projects.nyu.edu/wikis/docuwiki/doku.php/manual2">Communication</a>, and <a href="https://dev1.ed-projects.nyu.edu/wikis/docuwiki/doku.php/gesture">Gesture</a></li>
</ul></li>
<li>Enhancements to <a href="http://datavyu.org">Datavyu</a> for transcription, CHAT compatibility, Windows support</li>
</ul>
</section><section class="slide level2">

<div class="centered">
<p><img src="https://github.com/PLAY-behaviorome/video-coding/blob/master/video-code-file-export-cleaning_files/figure-markdown_github/loc-time-series-baby-1.png?raw=true" height=500px></p>
<p><a href="https://github.com/PLAY-behaviorome/video-coding" class="uri">https://github.com/PLAY-behaviorome/video-coding</a></p>
</div>
</section><section class="slide level2">

<div class="centered">
<p><img src="https://github.com/PLAY-behaviorome/video-coding/blob/master/video-code-file-export-cleaning_files/figure-markdown_github/voc.type-1.png?raw=true" height=500px></p>
<p><a href="https://github.com/PLAY-behaviorome/video-coding" class="uri">https://github.com/PLAY-behaviorome/video-coding</a></p>
</div>
</section><section class="slide level2">

<ul>
<li>Demographics + parent-report questionnaires about health, family, temperament</li>
<li>Ambient sound levels</li>
</ul>
</section><section class="slide level2">

<div class="centered">
<p><img src="https://github.com/PLAY-behaviorome/ambient-sound/blob/master/ambient-sound-ny-pilot_files/figure-markdown_github/plots-by-sub-db.avg-1.png?raw=true" height=500px></p>
</div>
</section><section class="slide level2">

<ul>
<li>Census block group geocoding</li>
</ul>
</section><section class="slide level2">

<div class="centered">
<p><img src="https://github.com/PLAY-behaviorome/site-demographics/blob/master/img/income-rent-region-1.png?raw=true" height=500px></p>
</div>
</section><section class="slide level2">

<div class="centered">
<p><img src="https://github.com/PLAY-behaviorome/site-demographics/blob/master/img/black-hispanic-region-1.png?raw=true" height=500px></p>
</div>
</section><section class="slide level2">

<ul>
<li>Data openly shared on Databrary
<ul>
<li>Adolph, K., Tamis-LeMonda, C. &amp; Gilmore, R.O. (2016). PLAY Project: Materials. Databrary. Retrieved January 24, 2018 from <a href="https://nyu.databrary.org/volume/254" class="uri">https://nyu.databrary.org/volume/254</a>.</li>
<li>Adolph, K., Tamis-LeMonda, C. &amp; Gilmore, R.O. (2017). PLAY Pilot Data Collections. Databrary. Retrieved January 24, 2018 from <a href="https://nyu.databrary.org/volume/444" class="uri">https://nyu.databrary.org/volume/444</a></li>
</ul></li>
</ul>
</section><section class="slide level2">

<ul>
<li>Video as data <strong>AND</strong> documentation</li>
</ul>
</section><section class="slide level2">

<div class="centered">
<p><a href="http://doi.org/10.1038/s41562-017-0128"> <img src="https://github.com/gilmore-lab/sips-2017-video-reproducibility/blob/master/img/gilmore-adolph-nat-hum-beh.jpg?raw=true" height=500px> </a></p>
<p><a href="http://doi.org/10.1038/s41562-017-0128">Gilmore &amp; Adolph 2017</a></p>
</div>
</section><section class="slide level2">

<div class="centered">
<p><a href="https://dev1.ed-projects.nyu.edu/wikis/docuwiki"> <img src="https://github.com/gilmore-lab/sips-2017-video-reproducibility/blob/master/img/play-wiki.jpg?raw=true" height=500px> </a></p>
</div>
</section><section id="what-questions-would-you-ask-about-these-sorts-of-data" class="slide level2">
<h2>What questions would <em>you</em> ask about these sorts of data?</h2>
</section><section id="how-could-the-data-be-made-maximimally-useful-to-other-researchers" class="slide level2">
<h2>How could the data be made maximimally useful to other researchers?</h2>
</section><section id="ideas-about-seeking-additional-funding" class="slide level2">
<h2>Ideas about seeking additional funding</h2>
</section></section>
<section><section id="how-infrastructure-can-enable-open-transparent-and-reproducible-big-data-developmental-science" class="titleslide slide level1" data-background="#999999"><h1>How infrastructure can enable open, transparent, and reproducible, “big data” developmental science</h1></section><section class="slide level2">

<iframe width="560" height="315" src="https://www.youtube.com/embed/pW6nZXeWlGM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
<p><a href="https://youtu.be/pW6nZXeWlGM" class="uri">https://youtu.be/pW6nZXeWlGM</a> <a href="https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation" class="uri">https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation</a></p>
</section><section class="slide level2">

<iframe width="560" height="315" src="https://www.youtube.com/embed/VOC3huqHrss" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
<p><a href="https://youtu.be/VOC3huqHrss" class="uri">https://youtu.be/VOC3huqHrss</a> <a href="https://pjreddie.com/darknet/yolo/" class="uri">https://pjreddie.com/darknet/yolo/</a></p>
</section><section class="slide level2">

<iframe width="560" height="315" src="https://www.youtube.com/embed/zEhlimS9feo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
<p><a href="https://www.youtube.com/watch?v=zEhlimS9feo#action=share" class="uri">https://www.youtube.com/watch?v=zEhlimS9feo#action=share</a></p>
</section><section class="slide level2">

<iframe width="560" height="315" src="https://www.youtube.com/embed/NRLlRh2apA8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</section><section class="slide level2">

<video controls>
<source src="https://nyu.databrary.org/slot/11680/0,24500/asset/41871/download?inline=true" type="video/mp4"> Your browser does not support the video tag.
</video>
<p>Jayaraman, S., et al. (2014). Natural Scene Statistics of Visual Experience Across Development and Culture. Databrary. Retrieved January 24, 2018 from <a href="http://doi.org/10.17910/B7988V" class="uri">http://doi.org/10.17910/B7988V</a></p>
</section><section class="slide level2">

<audio controls>
<source src="https://nbviewer.jupyter.org/github/gilmore-lab/pliers-play/blob/master/snd/peep-I-neu-chk.wav" type="audio/wav">
</audio>
<p></br> <audio controls> <source src="https://nbviewer.jupyter.org/github/gilmore-lab/pliers-play/blob/master/snd/peep-I-hap-tlk.wav" type="audio/wav"> </audio></p>
<p>Cole, P.M., Gilmore, R.O., Scherf, K.S. &amp; Perez-Edgar, K. (2016). The Proximal Emotional Environment Project (PEEP). Databrary. Retrieved January 24, 2018 from <a href="http://doi.org/10.17910/B7.248" class="uri">http://doi.org/10.17910/B7.248</a></p>
<p><a href="https://nbviewer.jupyter.org/github/gilmore-lab/pliers-play/blob/master/pliers-vision-speech-api-test.ipynb">Jupyter notebook</a></p>
</section><section id="from-static-repository-to-dynamic-analysis-platform" class="slide level2">
<h2>From static repository to dynamic analysis platform</h2>
</section><section id="nsf-research-implementations-for-data-intensive-research-in-the-social-behavioral-and-economic-sciences-ridir" class="slide level2">
<h2>NSF Research Implementations for Data Intensive Research in the Social, Behavioral, and Economic Sciences (RIDIR)</h2>
<p><a href="https://www.nsf.gov/pubs/2018/nsf18517/nsf18517.htm" class="uri">https://www.nsf.gov/pubs/2018/nsf18517/nsf18517.htm</a></p>
<p>3-4 awards, anticipated funding $4.5 M</p>
<p>Due February 28, 2018</p>
</section><section id="our-idea" class="slide level2">
<h2>Our idea</h2>
<h3 id="aim-1-enhance-databrarys-shared-video-audio-recordings-with-new-machine-generated-metadata">Aim 1: Enhance Databrary’s shared video &amp; audio recordings with new, machine-generated metadata</h3>
<h3 id="aim-2-create-secure-cloud-based-workspace-for-developing-and-testing-machine-learning-models-on-databrary-resources">Aim 2: Create secure, cloud-based workspace for developing and testing machine learning models on Databrary resources</h3>
<h3 id="aim-3-develop-robust-workflows-for-automated-gaze-direction-analysis-from-video">Aim 3: Develop robust workflows for automated gaze direction analysis from video</h3>
</section><section id="aim-1" class="slide level2">
<h2>Aim 1</h2>
<p>Collaborate with Tal Yarkoni and adapt his (NIH-funded) <a href="http://tyarkoni.github.io/pliers/index.html"><code>pliers</code></a> package</p>
<p>images/video: faces, objects, visual saliency, indoor/outdoor, text in image</p>
<p>sound: speech/non-speech, sound spectra</p>
</section><section class="slide level2">

<p>How to return ‘tags’ to Databrary in useful form? (time series + summary stats)</p>
<p>Offer tagging of unshared data volumes?</p>
<p>Privacy/confidentiality issues</p>
</section><section id="aim-2" class="slide level2">
<h2>Aim 2</h2>
<h3 id="cloud-based-workspace-for-analysis-visualization">Cloud-based workspace for analysis &amp; visualization</h3>
<h3 id="linked-to-databrary-files">Linked to Databrary files</h3>
<h3 id="facilitate-convenient-sshfs-or-similar-file-sharing-version-control">Facilitate convenient <code>sshfs</code> or similar file-sharing, version control</h3>
<h3 id="infrastructure-to-spawn-cloud-based-virtual-machines-to-manage-computationally-intensive-analyses">Infrastructure to spawn cloud-based virtual machines to manage computationally intensive analyses</h3>
<h3 id="ingest-session-metadata-from-spreadsheets">`Ingest’ session metadata from spreadsheets</h3>
<!-- ## Aim 3 -->
<!-- Collaborate with [Speech Recognition Virtual Kitchen](http://speechkitchen.org/) scientists -->
<!-- Apply sound categorization filters--"speech/non-speech" -->
<!-- Then speaker category--"adult/child" -->
<!-- Then speech-to-text on videos using [this tool or similar](https://github.com/srvk/eesen-transcriber) -->
</section><section id="aim-3" class="slide level2">
<h2>Aim 3</h2>
<p>Collaborate with Kim Scott (<a href="http://lookit.mit.edu">LookIt</a>), Rhodri Cusack and others</p>
<p>Develop and test new ML model from existing tagged training data + eye tracking</p>
</section><section class="slide level2">

<p><a href="http://cusacklab.s3.amazonaws.com/html/downloads/annotate3.mp4" class="uri">http://cusacklab.s3.amazonaws.com/html/downloads/annotate3.mp4</a></p>
<p>Source: Rhodri Cusack</p>
</section><section id="the-bigger-picture" class="slide level2">
<h2>The bigger picture</h2>
<p>Bring reproducible machine-assisted video/audio tagging to wider range of behavioral scientists</p>
<p>Make data sharing even more appealing, attractive, valuable</p>
<p>Make psychology a more cumulative science (<a href="https://www.psychologicalscience.org/observer/becoming-a-cumulative-science">Mischel 2009</a>)</p>
</section><section id="your-turn" class="slide level2">
<h2>Your turn</h2>
</section></section>
<section><section id="stack" class="titleslide slide level1"><h1>Stack</h1></section><section class="slide level2">

<p>This talk was produced on 2018-01-26 08:50:37 in <a href="http://rstudio.com">RStudio 1.1.383</a> using R Markdown and the reveal.JS framework. The code and materials used to generate the slides may be found at <a href="https://github.com/gilmore-lab/2018-01-26-p2c/" class="uri">https://github.com/gilmore-lab/2018-01-26-p2c/</a>. Information about the R Session that produced the slides is as follows:</p>
</section><section class="slide level2">

<pre><code>## R version 3.4.1 (2017-06-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## loaded via a namespace (and not attached):
##  [1] compiler_3.4.1  backports_1.1.0 magrittr_1.5    rprojroot_1.2  
##  [5] htmltools_0.3.6 tools_3.4.1     revealjs_0.9    yaml_2.1.14    
##  [9] Rcpp_0.12.12    stringi_1.1.5   rmarkdown_1.6   knitr_1.17     
## [13] stringr_1.2.0   digest_0.6.12   evaluate_0.10.1</code></pre>
</section></section>
    </div>
  </div>

  <script src="libs/reveal.js-3.3.0.1/lib/js/head.min.js"></script>
  <script src="libs/reveal.js-3.3.0.1/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display the page number of the current slide
        slideNumber: true,
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: true,
        // Opens links in an iframe preview overlay
        previewLinks: false,
        // Transition style
        transition: 'none', // none/fade/slide/convex/concave/zoom
        // Transition style for full page slide backgrounds
        backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom



        // Optional reveal.js plugins
        dependencies: [
          { src: 'libs/reveal.js-3.3.0.1/plugin/notes/notes.js', async: true },
          { src: 'libs/reveal.js-3.3.0.1/plugin/zoom-js/zoom.js', async: true },
        ]
      });
    </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

<script>
  (function() {
    if (window.jQuery) {
      Reveal.addEventListener( 'slidechanged', function(event) {  
        window.jQuery(event.previousSlide).trigger('hidden');
        window.jQuery(event.currentSlide).trigger('shown');
      });
    }
  })();
</script>


  </body>
</html>
